import json
from bs4 import BeautifulSoup
from urllib.parse import urljoin
from google.colab import files
import os

def locate_html_file():
    """–ü–æ–∏—Å–∫ HTML —Ñ–∞–π–ª–∞ –≤ —Ä–∞–±–æ—á–µ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏"""
    html_files = [f for f in os.listdir('.') if f.endswith('.html')]
    if html_files:
        print(f"–û–±–Ω–∞—Ä—É–∂–µ–Ω —Ñ–∞–π–ª: {html_files[0]}")
        return html_files[0]
    return None

def extract_party_data():
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –æ –ø–æ–ª–∏—Ç–∏—á–µ—Å–∫–∏—Ö –ø–∞—Ä—Ç–∏—è—Ö –∏–∑ HTML"""
    
    # –ü–æ–∏—Å–∫ HTML —Ñ–∞–π–ª–∞
    html_file = locate_html_file()
    
    if not html_file:
        print("HTML —Ñ–∞–π–ª –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏!")
        print("–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –∑–∞–≥—Ä—É–∑–∏—Ç–µ HTML —Ñ–∞–π–ª:")
        uploaded_files = files.upload()
        for filename in uploaded_files:
            if filename.endswith('.html'):
                html_file = filename
                break
    
    if not html_file:
        print("HTML —Ñ–∞–π–ª –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω")
        return []
    
    try:
        # –ó–∞–≥—Ä—É–∑–∫–∞ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ HTML —Ñ–∞–π–ª–∞
        with open(html_file, 'r', encoding='utf-8') as file:
            html_data = file.read()
        print(f"–§–∞–π–ª '{html_file}' –∑–∞–≥—Ä—É–∂–µ–Ω —É—Å–ø–µ—à–Ω–æ")
    except Exception as error:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ —Ñ–∞–π–ª–∞: {error}")
        return []
    
    soup = BeautifulSoup(html_data, 'html.parser')
    party_list = []
    
    
    # –ú–ï–¢–û–î 1: –ü–æ–∏—Å–∫ –≤ —Ç–∞–±–ª–∏—á–Ω—ã—Ö —Å—Ç—Ä—É–∫—Ç—É—Ä–∞—Ö
    table_elements = soup.find_all('table')
    
    for table in table_elements:
        data_rows = table.find_all('tr')[1:]  # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º —Å—Ç—Ä–æ–∫—É –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
        for row in data_rows:
            columns = row.find_all('td')
            if len(columns) >= 2:
                party_title = columns[0].get_text(strip=True)
                if party_title and len(party_title) > 8:
                    # –ü–æ–∏—Å–∫ —Å—Å—ã–ª–∫–∏ –Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç—ã
                    document_link = None
                    for column in columns:
                        anchor_tags = column.find_all('a', href=True)
                        for anchor in anchor_tags:
                            href_value = anchor.get('href')
                            if href_value and ('.pdf' in href_value.lower() or '/documents/' in href_value):
                                document_link = process_url(href_value)
                                break
                        if document_link:
                            break
                    
                    party_list.append({
                        'title': process_party_title(party_title),
                        'document_link': document_link
                    })
    
    # –ú–ï–¢–û–î 2: –ü–æ–∏—Å–∫ —á–µ—Ä–µ–∑ —Å—Å—ã–ª–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
    if not party_list:
        document_anchors = soup.find_all('a', href=lambda x: x and '/documents/' in x)
        print(f"–ù–∞–π–¥–µ–Ω–æ —Å—Å—ã–ª–æ–∫ –Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç—ã: {len(document_anchors)}")
        
        for anchor in document_anchors:
            party_title = anchor.get_text(strip=True)
            if party_title and len(party_title) > 8:
                document_link = process_url(anchor.get('href'))
                party_list.append({
                    'title': process_party_title(party_title),
                    'document_link': document_link
                })
    
    # –ú–ï–¢–û–î 3: –ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Å–ø–∏—Å–∫–æ–≤
    if not party_list:
        list_elements = soup.find_all(['ul', 'ol'])
        for list_item in list_elements:
            list_entries = list_item.find_all('li')
            for entry in list_entries:
                party_title = entry.get_text(strip=True)
                if party_title and len(party_title) > 8 and validate_party_name(party_title):
                    document_link = find_document_link(entry)
                    party_list.append({
                        'title': process_party_title(party_title),
                        'document_link': document_link
                    })
    
    # –ú–ï–¢–û–î 4: –ü–æ–∏—Å–∫ –ø–æ —ç–ª–µ–º–µ–Ω—Ç–∞–º —Å –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–º–∏ –∫–ª–∞—Å—Å–∞–º–∏
    if not party_list:
        div_elements = soup.find_all('div', class_=True)
        for div in div_elements:
            class_names = ' '.join(div.get('class', []))
            if any(term in class_names.lower() for term in ['organization', 'entry', 'file']):
                party_title = div.get_text(strip=True)
                if party_title and len(party_title) > 8 and validate_party_name(party_title):
                    document_link = find_document_link(div)
                    party_list.append({
                        'title': process_party_title(party_title),
                        'document_link': document_link
                    })
    
    # –£–¥–∞–ª–µ–Ω–∏–µ –ø–æ–≤—Ç–æ—Ä—è—é—â–∏—Ö—Å—è –∑–∞–ø–∏—Å–µ–π
    distinct_parties = []
    processed_titles = set()
    
    for party in party_list:
        if party['title'] not in processed_titles:
            processed_titles.add(party['title'])
            distinct_parties.append(party)
    
    # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –≤ –∞–ª—Ñ–∞–≤–∏—Ç–Ω–æ–º –ø–æ—Ä—è–¥–∫–µ
    distinct_parties.sort(key=lambda x: x['title'])
    
    return distinct_parties

def process_party_title(title):
    """–û–±—Ä–∞–±–æ—Ç–∫–∞ –∏ –æ—á–∏—Å—Ç–∫–∞ –Ω–∞–∑–≤–∞–Ω–∏—è –ø–∞—Ä—Ç–∏–∏"""
    if not title:
        return title
    
    # –£–¥–∞–ª–µ–Ω–∏–µ —Å–ª—É–∂–µ–±–Ω—ã—Ö –ø—Ä–µ—Ñ–∏–∫—Å–æ–≤
    prefix_list = [
        '–ü–æ–ª–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è',
        '–ó–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –ø–∞—Ä—Ç–∏—è',
        '–û—Ñ–∏—Ü–∏–∞–ª—å–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ',
        '–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ:',
        '–°–≤–∏–¥–µ—Ç–µ–ª—å—Å—Ç–≤–æ –æ —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏'
    ]
    
    for prefix in prefix_list:
        if title.startswith(prefix):
            title = title.replace(prefix, '').strip()
    
    # –£–¥–∞–ª–µ–Ω–∏–µ –ª–∏—à–Ω–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤ –∏ –ø—Ä–æ–±–µ–ª–æ–≤
    title = title.replace('"', '').replace('  ', ' ').strip()
    
    return title

def find_document_link(element):
    """–ü–æ–∏—Å–∫ —Å—Å—ã–ª–∫–∏ –Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç –≤ —ç–ª–µ–º–µ–Ω—Ç–µ"""
    if not element:
        return None
    
    # –ü–æ–∏—Å–∫ PDF –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
    pdf_anchors = element.find_all('a', href=lambda x: x and '.pdf' in x.lower())
    if pdf_anchors:
        return process_url(pdf_anchors[0].get('href'))
    
    # –ü–æ–∏—Å–∫ –ª—é–±—ã—Ö —Å—Å—ã–ª–æ–∫ –Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç—ã
    all_anchors = element.find_all('a', href=True)
    for anchor in all_anchors:
        href_value = anchor.get('href')
        if href_value and ('/documents/' in href_value or 'file' in href_value.lower()):
            return process_url(href_value)
    
    return None

def process_url(url):
    """–û–±—Ä–∞–±–æ—Ç–∫–∞ –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è URL"""
    if not url:
        return None
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –∞–±—Å–æ–ª—é—Ç–Ω–æ–≥–æ URL
    if url.startswith('/'):
        url = urljoin('https://minjust.gov.ru', url)
    
    # –ö–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∞ –ø—Ä–æ—Ç–æ–∫–æ–ª–∞
    if url.startswith('http://'):
        url = url.replace('http://', 'https://')
    
    return url

def validate_party_name(text):
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç–∏ –Ω–∞–∑–≤–∞–Ω–∏—è –ø–∞—Ä—Ç–∏–∏"""
    if not text or len(text) < 8 or len(text) > 200:
        return False
    
    key_terms = ['–ø–∞—Ä—Ç–∏—è', '—Ä–æ—Å—Å–∏—è', '–¥–µ–º–æ–∫—Ä–∞—Ç–∏—á', '—Å–æ—é–∑', '–¥–≤–∏–∂–µ–Ω–∏–µ', '–æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è']
    return any(term in text.lower() for term in key_terms)

def execute_parsing():
    # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
    parties_data = extract_party_data()
    
    if parties_data:
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ JSON —Ñ–æ—Ä–º–∞—Ç–µ
        with open('political_parties.json', 'w', encoding='utf-8') as f:
            json.dump(parties_data, f, ensure_ascii=False, indent=2)
        
        print(f"–û–±–Ω–∞—Ä—É–∂–µ–Ω–æ –ø–æ–ª–∏—Ç–∏—á–µ—Å–∫–∏—Ö –ø–∞—Ä—Ç–∏–π: {len(parties_data)}")
        print("–î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ —Ñ–∞–π–ª political_parties.json")
        
        # –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        print("–ü–ï–†–ï–ß–ï–ù–¨ –ü–û–õ–ò–¢–ò–ß–ï–°–ö–ò–• –ü–ê–†–¢–ò–ô:")
        for index, party in enumerate(parties_data, 1):
            doc_info = party['document_link'] if party['document_link'] else "–î–æ–∫—É–º–µ–Ω—Ç –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç"
            print(f"{index:2d}. {party['title']}")
            if party['document_link']:
                print(f"     üìé {party['document_link']}")
            print()
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
        print("–ó–∞–≥—Ä—É–∂–∞—é —Ñ–∞–π–ª —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏...")
        files.download('political_parties.json')
        
    else:
        print("–î–∞–Ω–Ω—ã–µ –æ –ø–∞—Ä—Ç–∏—è—Ö –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω—ã")

if __name__ == "__main__":
    execute_parsing()
